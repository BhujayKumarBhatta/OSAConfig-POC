---
repo_build_venv_timeout: 120
#lxc_net_mtu: "9000"
#lxc_container_default_mtu: "9000"

## set security hardening to false during upgrade
apply_security_hardening: True 
# chrony configuration is getting overwriiten by these settings
security_enable_chrony: false    
security_ntp_servers:
  - 10.0.0.51
  - 10.0.0.52
                   
## Debug and Verbose options.
debug: false
validate_certs: false
percona_package_download_validate_certs: false
haproxy_hatop_download_validate_certs: false
#pip_validate_certs: false
#galera_package_download_validate_certs: false
# use the keepalived  floating ip  of haproxy server as the proxy to avoid dependency on 52
proxy_env_url: "http://10.0.0.14:3128"
# no_proxy_env: "localhost,127.0.0.1,{{ internal_lb_vip_address }},{{ external_lb_vip_address }},{% for host in groups['all_containers'] %}{{ hostvars[host]['container_address'] }}{% if not loop.last %},{% endif %}{% endfor %}"
no_proxy_env: "localhost,127.0.0.1,10.0.0.51,10.0.0.52,10.0.0.21,10.0.0.22,10.0.0.23,10.0.0.54,{{ internal_lb_vip_address }},{{ external_lb_vip_address }},{% for host in groups['all_containers'] %}{{ hostvars[host]['container_address'] }}{% if not loop.last %},{% endif %}{% endfor %}"
#no_proxy_env: "localhost,127.0.0.1,,10.0.0.*,10.0.0.0,10.0.1.0,10.0.2.0"
global_environment_variables:
   HTTP_PROXY: "{{ proxy_env_url }}"
   HTTPS_PROXY: "{{ proxy_env_url }}"
   NO_PROXY: "{{ no_proxy_env }}"
   http_proxy: "{{ proxy_env_url }}"
   https_proxy: "{{ proxy_env_url }}"
   no_proxy: "{{ no_proxy_env }}"

deployment_environment_variables:
   HTTP_PROXY: "http://10.0.0.14:3128"
   HTTPS_PROXY: "http://10.0.0.14:3128"
   NO_PROXY: "localhost,127.0.0.1,10.0.0.14"
   http_proxy: "http://10.0.0.14:3128"
   https_proxy: "http://10.0.0.14:3128"
   no_proxy: "localhost,127.0.0.1,http://10.0.0.14:3128"

## SSH connection wait time
# If an increased delay for the ssh connection check is desired,# uncomment this variable and set it appropriately.   
ssh_delay: 60
#++++++++++++++++++Must for Self signed Certificate++++++++++++++++
keystone_ssl: false
keystone_external_ssl: false
keystone_service_adminuri_insecure: true
keystone_service_internaluri_insecure: true
#external_lb_vip_address: 14.142.104.140 its present in user_config
#internal_lb_vip_address: 10.0.0.14
# if using a self-signed certificate, set this to true to regenerate it
keystone_ssl_self_signed_regen: True
keystone_ssl_self_signed_subject: "/C=IN/ST=Karnataka/L=Bangalore/O=ITC/CN={{ internal_lb_vip_address }}/subjectAltName=IP.1={{ external_lb_vip_address }}"
keystone_ldap:
  ITC:
    url: "ldap://10.0.0.70"
    user: "cn=admin,dc=itc,dc=in"
    password: "welcome@123"
    user_tree_dn: "ou=OU1,dc=itc,dc=in"   
    user_objectclass: "inetOrgPerson"
    group_tree_dn: "cn=g1,ou=OU1,dc=itc,dc=in"
#      user_allow_create: "False"
#      user_allow_update: "False"
#      user_allow_delete: "False"
#      group_allow_create: "False"
#      group_allow_update: "False"
#      group_allow_delete: "False"
    user_id_attribute: "cn"
    user_name_attribute: "uid"
    
horizon_keystone_multidomain_support: True
#++++++++++++++++++++++++++++++++++++++
## haproxy default timeouts
haproxy_http_request_timeout: "10s"
haproxy_ssl_self_signed_regen: True
haproxy_ssl_self_signed_subject: "/C=IN/ST=Karnataka/L=Bangalore/O=ITC/CN={{ internal_lb_vip_address }}/subjectAltName=IP.1={{ external_lb_vip_address }}"

haproxy_keepalived_external_vip_cidr: 14.142.104.140/29
haproxy_keepalived_internal_vip_cidr: 10.0.0.14/24
haproxy_keepalived_external_interface: br-internet
haproxy_keepalived_internal_interface: br-mgmt
keepalived_ping_address: 10.0.0.54
#ocata
keepalived_use_latest_stable: True
#+++++++++++++++++Proxying TLS traffic often interferes with the clients ability to perform 
#successful validation of the certificate chain.Additional variables may exist
#within the project and will be named using the *_validate_certs pattern
pip_validate_certs: false
galera_package_download_validate_certs: false

# this is a place holder for nova config-drive , we will test it later

# configuration for nova, glance and cinder backup with ceph rbd as backend . Cinder volume config is in openstack_user_config

cephx: true
ceph_mons:
   - 10.0.2.193
   - 10.0.2.150
   - 10.0.2.118  
#mon_group_name: ceph-mon

# When nova_libvirt_images_rbd_pool is defined, ceph will be installed on nova
# hosts.
nova_libvirt_images_rbd_pool: vms
nova_force_config_drive: True

#nova_virt_type:
#  qemu:
#    nova_compute_driver: libvirt.LibvirtDriver
#    nova_scheduler_host_manager: host_manager
#    nova_reserved_host_memory_mb: 2048
#    nova_firewall_driver: nova.virt.firewall.NoopFirewallDriver
#    nova_scheduler_use_baremetal_filters: False
#    nova_scheduler_tracks_instance_changes: True
#    nova_cpu_mode: "none"

#nova_supported_virt_types:
#   - qemu
  

glance_default_store: rbd 
#glance_default_store: file

## Ceph pool name for Glance to use
glance_rbd_store_pool: images
glance_rbd_store_chunk_size: 8

cinder_service_backup_program_enabled: True 
#cinder_service_backup_driver: cinder.backup.drivers.ceph
#cinder_service_backup_ceph_user: cinder-backup
#cinder_service_backup_ceph_pool: backups

## ceilometer database mongodb manually installed and these settings done
#ceilometer_db_type: mongodb
#ceilometer_db_ip: 10.0.0.57
#ceilometer_db_port: 27017
gnocchi_storage_driver: ceph
gnocchi_ceph_pool: "gnocchi"
gnocchi_ceph_username: "gnocchi"

#swift_ceilometer_enabled: False

heat_ceilometer_enabled: True
cinder_ceilometer_enabled: True
glance_ceilometer_enabled: True
nova_ceilometer_enabled: True
neutron_ceilometer_enabled: True
keystone_ceilometer_enabled: True
#ceilometer_ceilometer_conf_overrides:
#  coordination:
#    backend_url: "memcached://10.0.0.151:11211"
#  notification:
#    workload_partitioning: True
#ceilometer_event_pipeline_yaml_overrides:
#  publishers:
#         - database://
#         - gnocchi://
#         - notifier://
#ceilometer_pipeline_yaml_overrides: https://docs.openstack.org/ocata/config-reference/telemetry/samples/event_pipeline.yaml.html
#ceilometer_aodh_enabled: false
#ceilometer_gnocchi_enabled: false

#https://docs.openstack.org/ocata/config-reference/telemetry/samples/event_definitions.yaml.html
#

horizon_enable_sahara_ui: True
horizon_enable_ironic_ui: True


# Enable/Disable designate
neutron_designate_enabled: True
neutron_dns_domain: itc.in. 
# Enable/Disable Ceilometer
neutron_ceilometer_enabled: True
neutron_plugin_base:
   - router
   - metering   
   - neutron_lbaas.services.loadbalancer.plugin.LoadBalancerPluginv2
   - firewall
   - qos
   - dns
#   - vpnaas
#   - neutron_dynamic_routing.services.bgp.bgp_plugin.BgpPlugin  

designate_pools_yaml:
   - name: "default"
     description: Default BIND9 Pool
     attributes: {}
     ns_records:
       - hostname: ns1.itc.in.
         priority: 1       
     nameservers:
       - host: 127.0.0.1
         port: 53
     targets:
       - type: bind9
         description: BIND9 Server
         masters:
           - host: 127.0.0.1
             port: 5354
         options:
           host: 127.0.0.1
           port: 53
           rndc_host: 127.0.0.1
           rndc_port: 953
swift:     
     part_power: 10
     weight: 100
     min_part_hours: 1
     repl_number: 3  
     storage_network: 'br-storage'
     replication_network: 'br-swift'     
     drives:
       - name: sdb
       - name: sdc
       - name: sdd
       - name: sde
       - name: sdf
     mount_point: /srv/node
     account: 
     container: 
     storage_policies:
      - policy:
         name: gold
         index: 0
         default: True
      - policy:
         name: silver
         index: 1
         repl_number: 3
         deprecated: True
swift_allow_all_users: True


#Swift Section ##################################################################################
dummy:
debian_package_dependencies:
  - python-pycurl
  - hdparm
  - chrony

ntp_service_enabled: false

debian_ceph_packages:
  - ceph
  - ceph-common    #|--> yes, they are already all dependencies from 'ceph'
#  - ceph-fs-common #|--> however while proceding to rolling upgrades and the 'ceph' package upgrade
  - ceph-fuse      #|--> they don't get update so we need to force them

ceph_origin: 'upstream' # or 'distro' or  'local'

# COMMUNITY VERSION
ceph_stable: true # use ceph stable branch
ceph_stable_release: kraken  # ceph stable release
#ceph_stable_repo: "{{ ceph_mirror }}/debian-{{ ceph_stable_release }}"
ceph_release_num:
#  dumpling: 0.67
#  emperor: 0.72
#  firefly: 0.80
#  giant: 0.87
#  hammer: 0.94
#  infernalis: 9
  jewel: 10
  kraken: 11.2.0

monitor_interface: eth2
monitor_address: 10.0.2.0/24

journal_size: 5120 # OSD journal size in MB
public_network: 10.0.2.0/24
cluster_network: 13.0.0.0/24
osd_mkfs_type: xfs

openstack_config: true
openstack_glance_pool:
  name: images
  pg_num: "{{ osd_pool_default_pg_num }}"
openstack_cinder_pool:
  name: volumes
  pg_num: "{{ osd_pool_default_pg_num }}"*
openstack_nova_pool:
  name: vms
  pg_num: "{{ osd_pool_default_pg_num }}"
openstack_cinder_backup_pool:
  name: backups
  pg_num: "{{ osd_pool_default_pg_num }}"
openstack_gnocchi_pool: 
  name: gnocchi
  pg_num: "{{ osd_pool_default_pg_num }}"
  
openstack_pools:
  - "{{ openstack_glance_pool }}"
  - "{{ openstack_cinder_pool }}"
  - "{{ openstack_nova_pool }}"
  - "{{ openstack_cinder_backup_pool }}"
  - "{{ openstack_gnocchi_pool }}"

openstack_keys:
  - { name: client.glance, value: "mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool={{ openstack_glance_pool.name }}'" }
  - { name: client.cinder, value: "mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool={{ openstack_cinder_pool.name }}, allow rwx pool={{ openstack_nova_pool.name }}, allow rx pool={{ openstack_glance_pool.name }}'"  }
  - { name: client.cinder-backup, value: "mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool={{ openstack_cinder_backup_pool.name }}'" }
  - { name: client.gnocchi, value: "mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool={{ openstack_gnocchi_pool.name }}'" }

devices:
  - /dev/sdb
#  - /dev/sdc
#  - /dev/sdd
#  - /dev/sde
journal_collocation: true










#galera_client_distro_packages:
#  - build-essential
#  - libaio1
#  - libc6
#  - libdbd-mysql-perl
#  - libgcc1
#  - libgcrypt20
#  - libmariadbclient-dev
#  - libssl-dev
#  - libstdc++6
#  - mariadb-client
##  - mariadb-client-core-10.0
#  - python-dev


#repo_build_upper_constraints_overrides:
##  # Due to bug  https://bugs.launchpad.net/openstack-ansible/+bug/1686483?comments=all
##  # trying with   
#  # Due to bug  https://bugs.launchpad.net/openstack-ansible/+bug/1686483?comments=all
#  # trying with   
#  - "sqlalchemy<1.1.0"

#aodh_requires_pip_packages:
#  - virtualenv
#  - virtualenv-tools
#  - python-keystoneclient # Keystoneclient needed to OSA keystone lib
#  - httplib2
#aodh_pip_packages: 
#  - alembic>=0.7.2
#  - aodh
#  - ceilometermiddleware
#  - gnocchiclient
#  - pycrypto
#  - PyMySQL>=0.6.2
#  - python-ceilometerclient
#  - python-memcached
#  - SQLAlchemy>=1.1.9
#  - sqlalchemy-utils
#  - warlock 
#
#ceilometer_requires_pip_packages:
#  - httplib2
#  - python-keystoneclient # Keystoneclient needed to OSA keystone lib
#  - virtualenv
#  - virtualenv-tools
#  - SQLAlchemy>=1.1.9

# Common pip packages
#aodh_pip_packages:
#  # The following constraints are taken from the setup.cfg
#  # file in the aodh project. This is due to the fact that the repo-build
#  # role does not respect constraints specified in setup.cfg files.
#  # https://github.com/openstack/aodh/blob/master/setup.cfg#L35-L38
#  - alembic>=0.7.2
#  - aodh
#  - ceilometermiddleware
#  - gnocchiclient
#  - pycrypto
#  - PyMySQL>=0.6.2
#  - python-ceilometerclient
#  - python-memcached
#  - sqlalchemy<1.1.0,>=0.9.7
#  - sqlalchemy-utils
#  - warlock
