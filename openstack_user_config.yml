---

cidr_networks:
   pod1_container: 10.0.0.0/24   
   pod1_storage: 20.0.0.0/24
   pod1_tunnel: 30.0.0.0/24
   
   pod2_container: 10.0.1.0/24   
   pod2_storage: 20.0.1.0/24
   pod2_tunnel: 30.0.1.0/24   

   brswift: 12.0.0.0/24
   brceph: 13.0.0.0/24

used_ips:
   - "10.0.0.1,10.0.0.100"
   - "20.0.0.1,20.0.0.100"
   - "30.0.0.1,30.0.0.100"
   - "10.0.1.1,10.0.1.100"
   - "20.0.1.1,20.0.1.100"
   - "30.0.1.1,30.0.1.100"
   - "12.0.0.1,12.0.0.100" 
   - "13.0.0.1,13.0.0.100"

pod1_hosts:
   haproxy1:
     ip: 10.0.0.51    
   haproxy2:
     ip: 10.0.0.52
   infra1:
    ip: 10.0.0.21
   infra2:
    ip: 10.0.0.22
   infra3:
    ip: 10.0.0.23  
   rsyslog1:
    ip: 10.0.0.57 
   kvm1:  
    ip: 10.0.0.31 
   
    
pod2_hosts:
   kvm1:     
    ip: 10.0.1.31   
#@   neutron1:
#@    ip: 10.0.1.41
    

global_overrides:
  internal_lb_vip_address: 10.0.0.14
  external_lb_vip_address: 14.142.104.140
  tunnel_bridge: "br-vxlan"
  management_bridge: "br-mgmt"
  provider_networks:
    - network:
        container_bridge: "br-mgmt"
        container_type: "veth"
        container_interface: "eth1"
        ip_from_q: "pod1_container"
        address_prefix: "container"        
        type: "raw"
        group_binds:
          - all_containers
          - hosts
        reference_group: "pod1_hosts"        
        is_container_address: true
        is_ssh_address: true
        static_routes:
          # Route to pod2 container from pod1 networks
          - cidr: 10.0.1.0/24
            gateway: 10.0.1.1    
             
    - network:
        container_bridge: "br-mgmt"
        container_type: "veth"
        container_interface: "eth1"
        ip_from_q: "pod2_container"
        address_prefix: "container"        
        type: "raw"
        group_binds:
          - all_containers
          - hosts
        reference_group: "pod2_hosts"        
        is_container_address: true
        is_ssh_address: true
        static_routes:
          # Route to pod1 container from pod2 networks
          - cidr: 10.0.0.0/24
            gateway: 10.0.0.1  
        
    - network:
        container_bridge: "br-vxlan"
        container_type: "veth"
        container_interface: "eth10"
        ip_from_q: "pod1_tunnel"
        reference_group: "pod1_hosts"   
        type: "vxlan"
        range: "1:1000"
        net_name: "vxlan"
        group_binds:
          - neutron_linuxbridge_agent
          
    - network:
        container_bridge: "br-vlan"
        container_type: "veth"
        container_interface: "eth12"        
        host_bind_override: "bond2" #?????? this ovverrides is trying to search eth2 on compute node  
        type: "flat"
        net_name: "flat"
        group_binds:
          - neutron_linuxbridge_agent          
          
    - network:
        container_bridge: "br-vlan"
        container_type: "veth"
        container_interface: "eth11"
        #container_mtu: "9000"
        type: "vlan"
        range: "1:1"
        net_name: "vlan"
        group_binds:
          - neutron_linuxbridge_agent
          
          
    - network:
        container_bridge: "br-storage"
        container_type: "veth"
        container_interface: "eth2"
        ip_from_q: "pod1_storage"
        reference_group: "pod1_hosts" 
        address_prefix: "storage"
        static_routes:
          # Route to pod2 container from pod1 networks
          - cidr: 20.0.1.0/24
            gateway: 20.0.1.1   
        #container_mtu: "9000"
        type: "raw"
        group_binds:
          - glance_api
          - cinder_api
          - cinder_volume          
          - swift_proxy
          - gnocchi_all
          - ceph-mon
          
    - network:
        container_bridge: "br-storage"
        container_type: "veth"
        container_interface: "eth2"
        ip_from_q: "pod2_storage"
        reference_group: "pod2_hosts" 
        static_routes:
          # Route to container networks
          - cidr: 20.0.0.0/24
            gateway: 20.0.0.1   
        #container_mtu: "9000"
        type: "raw"
        group_binds:          
          - nova_compute
            
     
    - network:
        container_bridge: "br-swift"
        container_type: "veth"
        container_interface: "eth3"
        ip_from_q: "brswift"
        container_mtu: "9000"
        type: "raw"
        group_binds:
          - swift_proxy
    - network:
        container_bridge: "br-ceph"
        container_type: "veth"
        container_interface: "eth3"
        ip_from_q: "brceph"
        container_mtu: "9000"
        type: "raw"
        group_binds:
          - ceph-mon
          
    
          

###
### Infrastructure
###
# The infra nodes that will be running the Unbound DNS caching resolvers
#unbound_hosts:
#  infra1:
#    ip: 10.0.0.21
#  infra2:
#    ip: 10.0.0.22
#  infra3:
#    ip: 10.0.0.23  
# load balancer

    
haproxy_hosts:
  haproxy1:
    ip: 10.0.0.51
    container_vars:
    #host_vars:
      global_environment_variables: 
      repo_pkg_cache_enabled: False
      resolvconf_enabled: False
  haproxy2:
    ip: 10.0.0.52
    container_vars:
    #host_vars:
      global_environment_variables: 
      repo_pkg_cache_enabled: False
      resolvconf_enabled: False
#    port: 2223
# rsyslog server
log_hosts:
# dont give the host name as rsyslog , playbook them 
#tries to deploy the server on bare metal and doesnt work proerly
  rsyslog1:
    ip: 10.0.0.57
# galera, memcache, rabbitmq, utility
shared-infra_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
  infra3:
    ip: 10.0.0.23  
# repository (apt cache, python packages, etc)
repo-infra_hosts:
#  infra1:
#    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
  infra3:
    ip: 10.0.0.23
# keystone
identity_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
  infra3:
    ip: 10.0.0.23  
# Barbican
#Xkey-manager_hosts: 
#X  infra1:
#X    ip: 10.0.0.21
#X  infra2:
#X    ip: 10.0.0.22
#X  infra3:
#X    ip: 10.0.0.23  
# cinder api services
storage-infra_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
  infra3:
    ip: 10.0.0.23  
# glance
image_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
  
# nova 
compute-infra_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
  infra3:
    ip: 10.0.0.23  
# heat
#Xorchestration_hosts:
#X  infra1:
#X    ip: 10.0.0.21
#X  infra2:
#X    ip: 10.0.0.22
# horizon
dashboard_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
  infra3:
    ip: 10.0.0.23  

   
# ceilometer (telemetry API)
#Xmetering-infra_hosts:
#X  infra1:
#X    ip: 10.0.0.21
  
# aodh (telemetry alarm service)
#Xmetering-alarm_hosts:
#X  infra2:
#X    ip: 10.0.0.22
  
# gnocchi (telemetry metrics storage)
#Xmetrics_hosts:
#X  infra3:
#X    ip: 10.0.0.23

# neutron server, agents (L3, etc)
network_hosts:
  infra1:
    ip: 10.0.0.21

####################################################################################################
# nova hypervisors
compute_hosts:
  kvm1: 
    container_vars:
      nova_virt_type: qemu 
    ip: 10.0.1.31
   
# ceilometer compute agent (telemetry)
#@metering-compute_hosts:
#@  kvm1:
#@    ip: 10.0.1.31

##########################CEPH STORAGE#############CEPH STORGE####################################
# The infra nodes where the Ceph mon services will run
ceph-mon_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
  infra3:
    ip: 10.0.0.23 

# The nodes that the Ceph OSD disks will be running on
ceph-osd_hosts:
  osd1:
    ip: 10.0.0.91
  osd2:
    ip: 10.0.0.92

# ########## SWIFT STORAGE ########################SWIFT STORAGE#######################
storage_hosts:
 infra1:
  ip: 10.0.0.21
  container_vars:
   cinder_backends:
     limit_container_types: cinder_volume
     rbd:
       volume_group: cinder-volumes
       volume_driver: cinder.volume.drivers.rbd.RBDDriver
       volume_backend_name: ceph
       rbd_pool: volumes
       rbd_ceph_conf: /etc/ceph/ceph.conf
       rbd_user: cinder
 infra2:
  ip: 10.0.0.22
  container_vars:
   cinder_backends:
     limit_container_types: cinder_volume
     rbd:
       volume_group: cinder-volumes
       volume_driver: cinder.volume.drivers.rbd.RBDDriver
       volume_backend_name: ceph
       rbd_pool: volumes
       rbd_ceph_conf: /etc/ceph/ceph.conf
       rbd_user: cinder
 infra3:
  ip: 10.0.0.23
  container_vars:
   cinder_backends:
     limit_container_types: cinder_volume
     rbd:
       volume_group: cinder-volumes
       volume_driver: cinder.volume.drivers.rbd.RBDDriver
       volume_backend_name: ceph
       rbd_pool: volumes
       rbd_ceph_conf: /etc/ceph/ceph.conf
       rbd_user: cinder
# storage1:
#  ip: 10.0.0.42
#  container_vars:
#   #is_metal: true
#   cinder_backends:
#     limit_container_types: cinder_volume
#     vnx5300:
#       volume_backend_name: vnx5300
#       #volume_driver: cinder.volume.drivers.emc.emc_cli_fc.EMCCLIFCDriver
#       volume_driver: cinder.volume.drivers.emc.vnx.driver.EMCVNXDriver
#       storage_protocol: fc
#       storage_vnx_pool_name: pool_0
#       initiator_auto_registration: True       
#       #storage_vnx_security_file_dir = /etc/secfile/array1
#       naviseccli_path: /opt/Navisphere/bin/naviseccli
#       default_timeout: 10
#       san_ip: 10.0.0.9
#       san_login: sysadmin
#       san_password:  Welcome@123?    
#       storage_vnx_authentication_type: global
#       destroy_empty_storage_group: False
#  limit_container_types: cinder_volume
  
##Swift-proxy
#swift-proxy_hosts:
#  infra1:
#    ip: 10.0.0.21
#  infra2:
#    ip: 10.0.0.22 
#  infra3:
#    ip: 10.0.0.23    
## Swift-Object-Storage-Nodes  
#swift_hosts:
#  swiftn1:
#    ip: 10.0.0.45
#  swiftn2:
#    ip: 10.0.0.46
##  swiftn3:
##    ip: 10.0.0.47  
 
#ocata
#       rbd_secret_uuid: "{{ cinder_ceph_client_uuid }}"


#storage_hosts:
#  storage1:
#    ip: 10.0.0.12
#    container_vars:
#      cinder_backends:
#        limit_container_types: cinder_volume
#        lvm:
#          volume_group: cinder-volumes
#          volume_driver: cinder.volume.drivers.lvm.LVMVolumeDriver
#          volume_backend_name: LVM_iSCSI
#          iscsi_ip_address: "10.0.0.12"
#storage_hosts:
#  storage1:
#    ip: 10.0.0.13
#    container_vars:
#      cinder_backends:
#        limit_container_types: cinder_volume
#        lvm:
#          volume_group: cinder-volumes
#          volume_driver: cinder.volume.drivers.lvm.LVMVolumeDriver
#          volume_backend_name: LVM_iSCSI
#          iscsi_ip_address: "10.0.0.12"
##storage_hosts:
#  infra2:
#    ip: 10.0.0.13
#    container_vars:
#      cinder_bacskends:
#        limit_container_types: cinder_volume
#        cinder_nfs_client:
#          nfs_shares_config: /etc/cinder/nfs_shares
#          shares:
#            - ip: "10.0.2.15"
#              share: "/vol/cinder"
#  infra1:
#    ip: 10.0.0.12
#    container_vars:
#      cinder_backends:
#        limit_container_types: cinder_volume
#        cinder_nfs_client:
#          nfs_shares_config: /etc/cinder/nfs_shares
#          shares:
#            - ip: "10.0.2.15"
#              share: "/vol/cinder"



# cinder storage host (ceph-backed) 