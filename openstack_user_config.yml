---

cidr_networks:
   container: 10.0.0.0/24
   tunnel: 10.0.1.0/24
   storage: 10.0.2.0/24

used_ips:
   - 10.0.0.100
   - "10.0.0.1,10.0.0.100"
   - "10.0.1.1,10.0.1.100"
   - "10.0.2.1,10.0.2.100"

global_overrides:
  internal_lb_vip_address: 10.0.0.14
  external_lb_vip_address: 14.142.104.140
  tunnel_bridge: "br-vxlan"
  management_bridge: "br-mgmt"
  provider_networks:
    - network:
        container_bridge: "br-mgmt"
        container_type: "veth"
        container_interface: "eth1"
      #  container_mtu: "9000"
        ip_from_q: "container"
        type: "raw"
        group_binds:
          - all_containers
          - hosts
        is_container_address: true
        is_ssh_address: true
    - network:
        container_bridge: "br-vxlan"
        container_type: "veth"
        container_interface: "eth10"
       # container_mtu: "9000"
        ip_from_q: "tunnel"
        type: "vxlan"
        range: "1:1000"
        net_name: "vxlan"
        group_binds:
          - neutron_linuxbridge_agent
    - network:
        container_bridge: "br-vlan"
        container_type: "veth"
        container_interface: "eth12"
        #container_mtu: "9000"
        host_bind_override: "bond2" #?????? this ovverrides is trying to search eth2 on compute node  
        type: "flat"
        net_name: "flat"
        group_binds:
          - neutron_linuxbridge_agent
    - network:
        container_bridge: "br-vlan"
        container_type: "veth"
        container_interface: "eth11"
        #container_mtu: "9000"
        type: "vlan"
        range: "1:1"
        net_name: "vlan"
        group_binds:
          - neutron_linuxbridge_agent
    - network:
        container_bridge: "br-storage"
        container_type: "veth"
        container_interface: "eth2"
        ip_from_q: "storage"
        #container_mtu: "9000"
        type: "raw"
        group_binds:
          - glance_api
          - cinder_api
          - cinder_volume
          - nova_compute
          - swift_proxy
          - gnocchi_all

###
### Infrastructure
###
# The infra nodes that will be running the Unbound DNS caching resolvers
#unbound_hosts:
#  infra1:
#    ip: 10.0.0.21
#  infra2:
#    ip: 10.0.0.22
#  infra3:
#    ip: 10.0.0.23  
# load balancer
haproxy_hosts:
  haproxy1:
    ip: 10.0.0.51
    container_vars:
    #host_vars:
      global_environment_variables: 
      repo_pkg_cache_enabled: False
      resolvconf_enabled: False
  haproxy2:
    ip: 10.0.0.52
    container_vars:
    #host_vars:
      global_environment_variables: 
      repo_pkg_cache_enabled: False
      resolvconf_enabled: False
#    port: 2223
# rsyslog server
log_hosts:
# dont give the host name as rsyslog , playbook them 
#tries to deploy the server on bare metal and doesnt work proerly
  rsyslog1:
    ip: 10.0.0.57
# galera, memcache, rabbitmq, utility
shared-infra_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
  infra3:
    ip: 10.0.0.23  
# repository (apt cache, python packages, etc)
repo-infra_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
###
### OpenStack
###
#os-infra_hosts:
# List of target hosts on which to deploy the glance API, nova API, heat API,
# and horizon.

# keystone
identity_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
  infra3:
    ip: 10.0.0.23  
# Barbican
key-manager_hosts: 
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
  infra3:
    ip: 10.0.0.23  
# cinder api services
storage-infra_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
  infra3:
    ip: 10.0.0.23  
# glance
image_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
  
# nova 
compute-infra_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
  infra3:
    ip: 10.0.0.23  
# heat
orchestration_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
# horizon
dashboard_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
# neutron server, agents (L3, etc)
network_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
  infra3:
    ip: 10.0.0.23  
# ceilometer (telemetry API)
metering-infra_hosts:
  infra1:
    ip: 10.0.0.21
  
# The infra nodes that will be running the dnsaas services
dnsaas_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22

# aodh (telemetry alarm service)
metering-alarm_hosts:
  infra1:
    ip: 10.0.0.21
  
# gnocchi (telemetry metrics storage)
metrics_hosts:
  infra1:
    ip: 10.0.0.21
  
# Bare metal api  
ironic-infra_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22  
# magnum api 
magnum-infra_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22  
# The infra nodes that will be running the trove services
trove-infra_hosts:
  infra1:
    ip: 10.0.0.21
#Sahara api
sahara-infra_hosts:
  infra1:
    ip: 10.0.0.21
  infra2:
    ip: 10.0.0.22
####################################################################################################
# nova hypervisors
compute_hosts:
  kvm1:  
    ip: 10.0.0.31
  kvm2:
    ip: 10.0.0.32   
# ceilometer compute agent (telemetry)
metering-compute_hosts:
  kvm1:
    ip: 10.0.0.31
  kvm2:  
    ip: 10.0.0.32
#  bm1:
#    ip: 10.0.0.42  

# Ironic compute hosts. These compute hosts will be used to
#  facilitate ironic's interactions through nova.
#ironic-compute_hosts:
#  bm1:
#    ip: 10.0.0.42
#    
#storage
storage_hosts:
 infra1:
  ip: 10.0.0.21
  container_vars:
   cinder_backends:
     limit_container_types: cinder_volume
     rbd:
       volume_group: cinder-volumes
       volume_driver: cinder.volume.drivers.rbd.RBDDriver
       volume_backend_name: ceph
       rbd_pool: volumes
       rbd_ceph_conf: /etc/ceph/ceph.conf
       rbd_user: cinder
 infra2:
  ip: 10.0.0.22
  container_vars:
   cinder_backends:
     limit_container_types: cinder_volume
     rbd:
       volume_group: cinder-volumes
       volume_driver: cinder.volume.drivers.rbd.RBDDriver
       volume_backend_name: ceph
       rbd_pool: volumes
       rbd_ceph_conf: /etc/ceph/ceph.conf
       rbd_user: cinder
 infra3:
  ip: 10.0.0.23
  container_vars:
   cinder_backends:
     limit_container_types: cinder_volume
     rbd:
       volume_group: cinder-volumes
       volume_driver: cinder.volume.drivers.rbd.RBDDriver
       volume_backend_name: ceph
       rbd_pool: volumes
       rbd_ceph_conf: /etc/ceph/ceph.conf
       rbd_user: cinder
 storage1:
  ip: 10.0.0.42
  container_vars:
   #is_metal: true
   cinder_backends:
     limit_container_types: cinder_volume
     vnx5300:
       volume_backend_name: vnx5300
       #volume_driver: cinder.volume.drivers.emc.emc_cli_fc.EMCCLIFCDriver
       volume_driver: cinder.volume.drivers.emc.vnx.driver.EMCVNXDriver
       storage_protocol: fc
       storage_vnx_pool_name: pool_1
       initiator_auto_registration: True       
       #storage_vnx_security_file_dir = /etc/secfile/array1
       naviseccli_path: /opt/Navisphere/bin/naviseccli
       default_timeout: 10
       san_ip: 10.0.0.90
       san_login: Sysadmin
       san_password:  Welcome@123     
       storage_vnx_authentication_type: global
       destroy_empty_storage_group: False
  limit_container_types: cinder_volume
#ocata
#       rbd_secret_uuid: "{{ cinder_ceph_client_uuid }}"


#storage_hosts:
#  storage1:
#    ip: 10.0.0.12
#    container_vars:
#      cinder_backends:
#        limit_container_types: cinder_volume
#        lvm:
#          volume_group: cinder-volumes
#          volume_driver: cinder.volume.drivers.lvm.LVMVolumeDriver
#          volume_backend_name: LVM_iSCSI
#          iscsi_ip_address: "10.0.0.12"
#storage_hosts:
#  storage1:
#    ip: 10.0.0.13
#    container_vars:
#      cinder_backends:
#        limit_container_types: cinder_volume
#        lvm:
#          volume_group: cinder-volumes
#          volume_driver: cinder.volume.drivers.lvm.LVMVolumeDriver
#          volume_backend_name: LVM_iSCSI
#          iscsi_ip_address: "10.0.0.12"
##storage_hosts:
#  infra2:
#    ip: 10.0.0.13
#    container_vars:
#      cinder_bacskends:
#        limit_container_types: cinder_volume
#        cinder_nfs_client:
#          nfs_shares_config: /etc/cinder/nfs_shares
#          shares:
#            - ip: "10.0.2.15"
#              share: "/vol/cinder"
#  infra1:
#    ip: 10.0.0.12
#    container_vars:
#      cinder_backends:
#        limit_container_types: cinder_volume
#        cinder_nfs_client:
#          nfs_shares_config: /etc/cinder/nfs_shares
#          shares:
#            - ip: "10.0.2.15"
#              share: "/vol/cinder"


# The infra nodes where the Ceph mon services will run
#ceph-mon_hosts:
#  infra1:
#    ip: 10.0.0.13
#  infra2:
#    ip: 10.0.0.15
#  infra3:
#    ip: 10.0.0.12 

# The nodes that the Ceph OSD disks will be running on
#ceph-osd_hosts:
#  aio1:
#    ip: 172.29.236.100
#  osd1:
#    ip: 10.0.0.84
#  osd2:
#    ip: 10.0.0.86
#  osd3:
#    ip: 10.0.0.87
#  osd5:
#    ip: 10.0.0.85
# cinder storage host (ceph-backed) 